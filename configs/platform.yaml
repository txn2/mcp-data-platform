# MCP Data Platform Configuration
# Copy this file and customize for your environment

apiVersion: v1

# Config store mode: "file" (default, read-only) or "database" (persistent mutations)
# In file mode, config mutations (persona CRUD, auth key management) are blocked.
# In database mode, mutations are persisted to PostgreSQL and survive restarts.
# config_store:
#   mode: file

server:
  name: mcp-data-platform
  transport: stdio              # stdio, http
  address: ":8080"
  tls:
    enabled: false
    cert_file: /etc/ssl/server.crt
    key_file: /etc/ssl/server.key
  streamable:
    session_timeout: 30m        # idle session timeout (default: 30m)
    stateless: false            # disable session tracking
  shutdown:
    grace_period: 25s           # max time to drain in-flight requests (default: 25s)
    pre_shutdown_delay: 2s      # sleep before draining for LB deregistration (default: 2s)

# Session externalization for zero-downtime upgrades
# sessions:
#   store: memory               # "memory" (default) or "database"
#   ttl: 30m                    # session lifetime (defaults to streamable.session_timeout)
#   idle_timeout: 30m           # idle session eviction (defaults to streamable.session_timeout)
#   cleanup_interval: 1m        # cleanup routine frequency

# Generic OIDC authentication
auth:
  oidc:
    enabled: false
    issuer: "https://auth.example.com/realms/platform"
    client_id: "mcp-data-platform"
    audience: "mcp-data-platform"
    role_claim_path: "realm_access.roles"
    role_prefix: "dp_"

  api_keys:
    enabled: true
    keys:
      - key: "${API_KEY_ADMIN}"
        name: "admin"
        roles: ["admin"]

# OAuth 2.1 server (for MCP clients like Claude)
oauth:
  enabled: false
  dcr:
    enabled: true
    allowed_redirect_patterns:
      - "https://.*\\.anthropic\\.com/callback"
      - "http://localhost:.*/callback"

# PostgreSQL for persistence
database:
  dsn: "${DATABASE_URL}"
  max_open_conns: 25

# Persona definitions
personas:
  analyst:
    display_name: "Data Analyst"
    roles:
      - analyst
      - data_engineer
    tools:
      allow:
        - "trino_*"
        - "datahub_*"
        - "s3_list_*"
      deny:
        - "s3_delete_*"
    prompts:
      system_prefix: |
        You are helping a data analyst explore and query data.
        Always check DataHub for context before writing SQL.
    hints:
      datahub_search: "Start here to discover datasets"
      trino_describe_table: "Get schema with business context"

  executive:
    display_name: "Executive"
    roles:
      - executive
      - business_analyst
    tools:
      allow:
        - "datahub_search"
        - "datahub_get_entity"
        - "datahub_list_*"
      deny: []
    prompts:
      system_prefix: |
        You are providing business insights to an executive.
        Focus on what data means, not technical details.

  admin:
    display_name: "Administrator"
    roles:
      - admin
    tools:
      allow:
        - "*"
      deny: []

  default_persona: analyst

  role_mapping:
    oidc_to_persona:
      "dp_admin": "admin"
      "dp_analyst": "analyst"
      "dp_executive": "executive"
    user_personas:
      "admin@example.com": "admin"

# Tool visibility filter and description overrides
# Reduces token usage by hiding tools from tools/list responses.
# This is a visibility filter, not a security boundary — persona auth
# continues to gate tools/call independently.
# tools:
#   allow:
#     - "trino_*"
#     - "datahub_*"
#   deny:
#     - "*_delete_*"
#
#   # Override tool descriptions in tools/list to guide agent behavior.
#   # Built-in overrides for trino_query and trino_execute already encourage
#   # calling datahub_search first. Use this to customize or add more.
#   description_overrides:
#     trino_query: "Custom description for trino_query..."
#     my_custom_tool: "Explain what this tool does"

# Toolkit instances
toolkits:
  trino:
    enabled: false
    instances:
      production:
        host: "trino.example.com"
        port: 443
        user: "${TRINO_USER}"
        password: "${TRINO_PASSWORD}"
        catalog: "iceberg"
        ssl: true
        description: "Production data warehouse for batch analytics and reporting"
      staging:
        host: "trino-staging.example.com"
        port: 8080
        description: "Staging environment for testing queries before production"
    default: production
    config:
      default_limit: 1000
      max_limit: 10000
      read_only: true
      # descriptions:
      #   trino_query: "Execute a SQL query against the data warehouse"
      #   trino_describe_table: "Get table schema with column types and sample data"

  datahub:
    enabled: false
    instances:
      primary:
        endpoint: "https://datahub.example.com/api/graphql"
        token: "${DATAHUB_TOKEN}"
        debug: false            # Enable debug logging for GraphQL operations
    default: primary
    config:
      search_limit: 50
      lineage_max_depth: 5
      # descriptions:
      #   datahub_search: "Search the data catalog for datasets and dashboards"
      #   datahub_get_entity: "Get full metadata for a catalog entity by URN"

  s3:
    enabled: false
    instances:
      data_lake:
        region: "us-east-1"
        bucket_prefix: "data-lake-"
    default: data_lake
    config:
      read_only: true

# Semantic layer (REQUIRED)
semantic:
  provider: noop              # datahub, noop
  instance: primary
  cache:
    enabled: true
    ttl: 5m

  # URN mapping translates between query engine names and metadata catalog names.
  # This is necessary when Trino uses different catalog/platform names than DataHub.
  #
  # Example: Trino queries "rdbms.public.users" but DataHub stores metadata as
  # "urn:li:dataset:(urn:li:dataPlatform:postgres,warehouse.public.users,PROD)"
  #
  # Configure urn_mapping to bridge this gap:
  urn_mapping:
    # Platform name used in DataHub URNs (e.g., "postgres", "mysql", "trino")
    # If Trino connects to PostgreSQL, set this to "postgres" to match DataHub's platform
    platform: ""

    # Map Trino catalog names to DataHub catalog names
    # Keys are Trino catalogs, values are DataHub catalogs
    catalog_mapping: {}
    # Example:
    # catalog_mapping:
    #   rdbms: warehouse      # Trino "rdbms" → DataHub "warehouse"
    #   iceberg: datalake     # Trino "iceberg" → DataHub "datalake"

  # Lineage-aware semantic enrichment inherits metadata from upstream datasets
  # when columns lack documentation. See docs/lineage.md for details.
  lineage:
    enabled: false            # Enable lineage-aware column inheritance
    max_hops: 2               # Max upstream traversal depth (1-5)
    inherit:                  # Metadata types to inherit
      - glossary_terms
      - descriptions
      # - tags                # Uncomment to also inherit tags
    conflict_resolution: nearest  # nearest, all, skip
    prefer_column_lineage: true   # Use column-level lineage when available
    # column_transforms:      # Path normalization rules
    #   - target_pattern: "*_flattened"
    #     strip_prefix: "payload."
    # aliases:                # Explicit source-target mappings
    #   - source: "warehouse.raw.events"
    #     targets: ["warehouse.analytics.*"]
    cache_ttl: 10m
    timeout: 5s

# Query provider
query:
  provider: noop              # trino, noop
  instance: production

  # URN mapping for DataHub → Trino direction (reverse of semantic urn_mapping)
  # Maps DataHub catalog names back to Trino catalog names for query resolution
  urn_mapping:
    catalog_mapping: {}
    # Example (reverse of semantic mapping):
    # catalog_mapping:
    #   warehouse: rdbms      # DataHub "warehouse" → Trino "rdbms"
    #   datalake: iceberg     # DataHub "datalake" → Trino "iceberg"

# Cross-injection
injection:
  trino_semantic_enrichment: true
  datahub_query_enrichment: true

  # Search schema preview
  # Adds a bounded column-name+type preview to datahub_search query_context
  # for available tables, so agents can write SQL without an intermediate
  # datahub_get_schema or trino_describe_table call.
  # search_schema_preview: true            # Default: true
  # schema_preview_max_columns: 15         # Default: 15

  # Column context filtering
  # Limits column-level semantic enrichment to only columns referenced
  # in the SQL query. Saves tokens when queries touch a subset of wide tables.
  # Only applies to SQL queries (trino_query); trino_describe_table always
  # shows all columns. PII/sensitive columns are always included for safety.
  # column_context_filtering: true   # Default: true

  # Session metadata deduplication
  # Avoids repeating large semantic metadata blocks for previously-enriched
  # tables within the same client session, saving LLM context tokens.
  session_dedup:
    enabled: true               # Default: true (dedup is on by default)
    mode: reference             # reference (default), summary, none
    # entry_ttl: 5m             # How long a table stays "already sent" (defaults to semantic.cache.ttl)
    # session_timeout: 30m      # Idle session cleanup (defaults to server.streamable.session_timeout)

# Workflow gating
# Session-aware enforcement that agents call DataHub discovery tools before
# running Trino queries. When enabled, query results are annotated with
# warnings if no discovery has occurred in the current session.
# workflow:
#   require_discovery_before_query: true
#   # discovery_tools: []           # Defaults to all datahub_* tools
#   # query_tools: []               # Defaults to trino_query, trino_execute
#   # warning_message: ""           # Custom warning (default: built-in REQUIRED message)
#   escalation:
#     after_warnings: 3             # Switch to escalated message after N warnings
#     # escalation_message: ""      # Custom escalation (use {count} for warning number)

# Tuning
tuning:
  rules:
    require_datahub_check: true
    warn_on_deprecated: true
    quality_threshold: 0.7
  prompts_dir: ""

# Knowledge capture and apply
# Enables the capture_insight tool for recording domain knowledge.
# Insights are stored in the database for admin review.
# knowledge:
#   enabled: true
#   apply:
#     enabled: true                # Enable apply_knowledge tool (default: false)
#     datahub_connection: primary  # Which DataHub instance to write to
#     require_confirmation: true   # Require confirm: true on apply (default: false)

# Admin REST API
# admin:
#   enabled: true
#   portal: true                # enable admin UI portal (default: false)
#   persona: admin              # persona required for admin access (default: "admin")
#   path_prefix: /api/v1/admin  # URL prefix for admin endpoints
#   portal_title: "My Data Platform"                        # sidebar title (default: "Admin Portal")
#   portal_logo: https://example.com/logo.svg               # logo URL, used if no light/dark specified
#   portal_logo_light: https://example.com/logo-dark.svg   # logo for light theme
#   portal_logo_dark: https://example.com/logo-white.svg   # logo for dark theme

# Resource templates (RFC 6570 URI templates)
# Exposes platform data as browseable, parameterized MCP resources.
# Templates: schema://{catalog}.{schema}/{table}, glossary://{term},
#            availability://{catalog}.{schema}/{table}
# resources:
#   enabled: true
#
#   # User-defined static resources available to MCP clients and agents.
#   # Registered whenever non-empty, independent of resources.enabled.
#   # Use inline content (content:) or a file path (content_file:) — not both.
#   custom:
#     - uri: "brand://theme"
#       name: "Brand Theme"
#       description: "Primary brand colors, logo SVG, and site URL"
#       mime_type: "application/json"
#       content: |
#         {
#           "colors": {"primary": "#FF6B35", "secondary": "#004E89"},
#           "url": "https://example.com"
#         }
#     - uri: "brand://logo"
#       name: "Brand Logo SVG"
#       mime_type: "image/svg+xml"
#       content_file: "/etc/platform/logo.svg"

# Progress notifications
# Sends granular progress updates to MCP clients during long-running
# Trino queries (requires client to send _meta.progressToken).
# progress:
#   enabled: true

# Client logging
# Sends server-to-client log messages (enrichment decisions, timing)
# via MCP logging/setLevel protocol. Zero overhead if client hasn't
# called setLevel.
# client_logging:
#   enabled: true

# Elicitation (user confirmation prompts)
# Requests user confirmation before expensive queries or PII access.
# Requires client-side elicitation support (e.g., Claude Desktop).
# Gracefully degrades to no-op if the client doesn't support elicitation.
# elicitation:
#   enabled: true
#   cost_estimation:
#     enabled: true
#     row_threshold: 1000000    # prompt when EXPLAIN IO estimates > 1M rows
#   pii_consent:
#     enabled: true             # prompt when query accesses PII-tagged columns

# Icons (tool/resource/prompt visual metadata)
# Override or add icons to MCP list responses.
# Upstream toolkits provide default icons; use this to customize.
# icons:
#   enabled: true
#   tools:
#     trino_query:
#       src: "https://example.com/custom-trino.svg"
#       mime_type: "image/svg+xml"
#   resources:
#     "schema://{catalog}.{schema}/{table}":
#       src: "https://example.com/schema.svg"
#   prompts:
#     knowledge_capture:
#       src: "https://example.com/knowledge.svg"

# Audit logging
audit:
  enabled: false
  log_tool_calls: true
  retention_days: 90
