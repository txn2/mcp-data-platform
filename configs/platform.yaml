# MCP Data Platform Configuration
# Copy this file and customize for your environment

apiVersion: v1

server:
  name: mcp-data-platform
  transport: stdio              # stdio, http
  address: ":8080"
  tls:
    enabled: false
    cert_file: /etc/ssl/server.crt
    key_file: /etc/ssl/server.key
  streamable:
    session_timeout: 30m        # idle session timeout (default: 30m)
    stateless: false            # disable session tracking
  shutdown:
    grace_period: 25s           # max time to drain in-flight requests (default: 25s)
    pre_shutdown_delay: 2s      # sleep before draining for LB deregistration (default: 2s)

# Session externalization for zero-downtime upgrades
# sessions:
#   store: memory               # "memory" (default) or "database"
#   ttl: 30m                    # session lifetime (defaults to streamable.session_timeout)
#   idle_timeout: 30m           # idle session eviction (defaults to streamable.session_timeout)
#   cleanup_interval: 1m        # cleanup routine frequency

# Generic OIDC authentication
auth:
  oidc:
    enabled: false
    issuer: "https://auth.example.com/realms/platform"
    client_id: "mcp-data-platform"
    audience: "mcp-data-platform"
    role_claim_path: "realm_access.roles"
    role_prefix: "dp_"

  api_keys:
    enabled: true
    keys:
      - key: "${API_KEY_ADMIN}"
        name: "admin"
        roles: ["admin"]

# OAuth 2.1 server (for MCP clients like Claude)
oauth:
  enabled: false
  dcr:
    enabled: true
    allowed_redirect_patterns:
      - "https://.*\\.anthropic\\.com/callback"
      - "http://localhost:.*/callback"

# PostgreSQL for persistence
database:
  dsn: "${DATABASE_URL}"
  max_open_conns: 25

# Persona definitions
personas:
  analyst:
    display_name: "Data Analyst"
    roles:
      - analyst
      - data_engineer
    tools:
      allow:
        - "trino_*"
        - "datahub_*"
        - "s3_list_*"
      deny:
        - "s3_delete_*"
    prompts:
      system_prefix: |
        You are helping a data analyst explore and query data.
        Always check DataHub for context before writing SQL.
    hints:
      datahub_search: "Start here to discover datasets"
      trino_describe_table: "Get schema with business context"

  executive:
    display_name: "Executive"
    roles:
      - executive
      - business_analyst
    tools:
      allow:
        - "datahub_search"
        - "datahub_get_entity"
        - "datahub_list_*"
      deny: []
    prompts:
      system_prefix: |
        You are providing business insights to an executive.
        Focus on what data means, not technical details.

  admin:
    display_name: "Administrator"
    roles:
      - admin
    tools:
      allow:
        - "*"
      deny: []

  default_persona: analyst

  role_mapping:
    oidc_to_persona:
      "dp_admin": "admin"
      "dp_analyst": "analyst"
      "dp_executive": "executive"
    user_personas:
      "admin@example.com": "admin"

# Toolkit instances
toolkits:
  trino:
    enabled: false
    instances:
      production:
        host: "trino.example.com"
        port: 443
        user: "${TRINO_USER}"
        password: "${TRINO_PASSWORD}"
        catalog: "iceberg"
        ssl: true
      staging:
        host: "trino-staging.example.com"
        port: 8080
    default: production
    config:
      default_limit: 1000
      max_limit: 10000
      read_only: true

  datahub:
    enabled: false
    instances:
      primary:
        endpoint: "https://datahub.example.com/api/graphql"
        token: "${DATAHUB_TOKEN}"
        debug: false            # Enable debug logging for GraphQL operations
    default: primary
    config:
      search_limit: 50
      lineage_max_depth: 5

  s3:
    enabled: false
    instances:
      data_lake:
        region: "us-east-1"
        bucket_prefix: "data-lake-"
    default: data_lake
    config:
      read_only: true

# Semantic layer (REQUIRED)
semantic:
  provider: noop              # datahub, noop
  instance: primary
  cache:
    enabled: true
    ttl: 5m

  # URN mapping translates between query engine names and metadata catalog names.
  # This is necessary when Trino uses different catalog/platform names than DataHub.
  #
  # Example: Trino queries "rdbms.public.users" but DataHub stores metadata as
  # "urn:li:dataset:(urn:li:dataPlatform:postgres,warehouse.public.users,PROD)"
  #
  # Configure urn_mapping to bridge this gap:
  urn_mapping:
    # Platform name used in DataHub URNs (e.g., "postgres", "mysql", "trino")
    # If Trino connects to PostgreSQL, set this to "postgres" to match DataHub's platform
    platform: ""

    # Map Trino catalog names to DataHub catalog names
    # Keys are Trino catalogs, values are DataHub catalogs
    catalog_mapping: {}
    # Example:
    # catalog_mapping:
    #   rdbms: warehouse      # Trino "rdbms" → DataHub "warehouse"
    #   iceberg: datalake     # Trino "iceberg" → DataHub "datalake"

  # Lineage-aware semantic enrichment inherits metadata from upstream datasets
  # when columns lack documentation. See docs/lineage.md for details.
  lineage:
    enabled: false            # Enable lineage-aware column inheritance
    max_hops: 2               # Max upstream traversal depth (1-5)
    inherit:                  # Metadata types to inherit
      - glossary_terms
      - descriptions
      # - tags                # Uncomment to also inherit tags
    conflict_resolution: nearest  # nearest, all, skip
    prefer_column_lineage: true   # Use column-level lineage when available
    # column_transforms:      # Path normalization rules
    #   - target_pattern: "*_flattened"
    #     strip_prefix: "payload."
    # aliases:                # Explicit source-target mappings
    #   - source: "warehouse.raw.events"
    #     targets: ["warehouse.analytics.*"]
    cache_ttl: 10m
    timeout: 5s

# Query provider
query:
  provider: noop              # trino, noop
  instance: production

  # URN mapping for DataHub → Trino direction (reverse of semantic urn_mapping)
  # Maps DataHub catalog names back to Trino catalog names for query resolution
  urn_mapping:
    catalog_mapping: {}
    # Example (reverse of semantic mapping):
    # catalog_mapping:
    #   warehouse: rdbms      # DataHub "warehouse" → Trino "rdbms"
    #   datalake: iceberg     # DataHub "datalake" → Trino "iceberg"

# Cross-injection
injection:
  trino_semantic_enrichment: true
  datahub_query_enrichment: true

  # Session metadata deduplication
  # Avoids repeating large semantic metadata blocks for previously-enriched
  # tables within the same client session, saving LLM context tokens.
  session_dedup:
    enabled: true               # Default: true (dedup is on by default)
    mode: reference             # reference (default), summary, none
    # entry_ttl: 5m             # How long a table stays "already sent" (defaults to semantic.cache.ttl)
    # session_timeout: 30m      # Idle session cleanup (defaults to server.streamable.session_timeout)

# Tuning
tuning:
  rules:
    require_datahub_check: true
    warn_on_deprecated: true
    quality_threshold: 0.7
  prompts_dir: ""

# Knowledge capture
# Enables the capture_insight tool for recording domain knowledge.
# Insights are stored in the database for admin review.
# knowledge:
#   enabled: true

# Audit logging
audit:
  enabled: false
  log_tool_calls: true
  retention_days: 90
