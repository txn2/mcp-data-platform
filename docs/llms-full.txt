# mcp-data-platform

> MCP server for AI-assisted data exploration. DataHub semantic layer with optional Trino and S3. Cross-injection automatically enriches query results with business context: owners, tags, quality scores, deprecation warnings. Implements fail-closed security with OIDC/API key authentication, TLS for HTTP transport, and prompt injection protection.

For the security architecture rationale, see: https://imti.co/mcp-defense/

---

# Overview

Your AI assistant can run SQL. But it doesn't know that `cust_id` contains PII, that the table was deprecated last month, or who to ask when something breaks.

mcp-data-platform fixes that. It connects AI assistants to your data infrastructure and adds business context from your semantic layer. Query a table and get its meaning, owners, quality scores, and deprecation warnings in the same response.

The only requirement is DataHub (https://datahubproject.io/). Add Trino (https://trino.io/) for SQL queries and S3 for object storage when you're ready.

## Key Features

- **Semantic-First**: DataHub is the foundation. Query a table, get its business context automatically: owners, tags, quality scores, deprecation warnings. No separate lookups.

- **Cross-Injection**: Trino results include DataHub metadata. DataHub searches show which datasets are queryable. Context flows between services automatically.

- **Enterprise Security**: Fail-closed authentication model, TLS enforcement for HTTP transport, prompt injection protection, and read-only mode enforcement.

- **Built for Customization**: Add custom toolkits, providers, and middleware. The Go library exposes everything. Build the data platform your organization needs.

- **Personas**: Define who can use which tools. Analysts get read access. Admins get everything. Map from your identity provider's roles.

---

# The Data Stack: DataHub + Trino + S3

Modern data platforms need three things: meaning (what does the data represent?), access (how do I query it?), and storage (where does it live?). mcp-data-platform uses DataHub for meaning, Trino for access, and S3 for storage.

## DataHub: The Semantic Layer

DataHub (https://datahubproject.io/) is an open source data catalog from LinkedIn. It stores business context: descriptions, owners, tags, glossary terms, lineage, and quality scores.

**Problem it solves**: Data exists everywhere, but understanding what it means requires tribal knowledge. Column `cid` in one system is `customer_id` in another. That deprecated table still gets queried because nobody knows it's deprecated.

**Why DataHub**: Active community (10k+ GitHub stars), rich GraphQL/REST APIs, 50+ integrations (Trino, Snowflake, dbt, Airflow), real-time ingestion, full-text search, lineage tracking.

**For AI**: Without DataHub, an AI sees columns and types. With DataHub, it sees what the data means, who owns it, and whether it's reliable.

## Trino: Universal SQL Access

Trino (https://trino.io/, formerly PrestoSQL from Facebook) is a distributed SQL query engine. It runs SQL against data where it lives, without moving it first.

**Trino connects to everything**: PostgreSQL, MySQL, Oracle, Snowflake, BigQuery, Elasticsearch, MongoDB, S3, HDFS, Delta Lake, Iceberg, Kafka, and more.

**One SQL dialect**: Query across PostgreSQL, Elasticsearch, and S3 in one statement. The AI doesn't need to know where data lives. It's all SQL.

**Why Trino**: Battle-tested at Meta, Netflix, Uber, LinkedIn. Cost-based optimizer, standard ANSI SQL, federated queries without data movement.

## S3: The Universal Data Lake

S3 (Amazon S3, MinIO, or any S3-compatible service) is object storage for any data: files, Parquet, JSON, logs, ML models.

**Problem it solves**: Not all data is structured or in databases. Data platforms must handle structured (tables), semi-structured (JSON, Parquet), and unstructured (PDFs, images) data.

**Why S3**: Infinite scale at low cost, any file format, direct querying via Trino, object versioning, S3-compatible (AWS, MinIO, Ceph).

## Cross-injection fills the gaps

| Component | Answers | Limitation Alone |
|-----------|---------|------------------|
| DataHub | "What does this mean?" | Can't query the data |
| Trino | "What's in this table?" | Doesn't know business context |
| S3 | "What files exist?" | Just storage, no meaning |

Cross-injection wires them together:
- Trino + DataHub: Query a table → Get schema + owners + tags + deprecation + quality
- DataHub + Trino: Search DataHub → See which datasets are queryable with sample SQL
- S3 + DataHub: List objects → Get matching metadata and ownership

This stack is built for OLAP: S3 stores petabytes at low cost, Trino runs analytical queries across that data, DataHub adds business context.

---

## What's Included

| Toolkit | Tools | Required |
|---------|-------|----------|
| DataHub | 11 tools | Yes |
| Trino | 7 tools | No |
| S3 | 6-9 tools | No |
| Knowledge | 1-2 tools | No |

---

# Installation

## Prerequisites

- Go 1.24+ (for building from source)
- An MCP-compatible client (Claude Desktop, Claude Code, or custom)
- Access to Trino, DataHub, and/or S3 services you want to connect

## Installation Methods

### Go Install

```bash
go install github.com/txn2/mcp-data-platform/cmd/mcp-data-platform@latest
```

### Homebrew (macOS)

```bash
brew install txn2/tap/mcp-data-platform
```

### Docker

```bash
docker pull ghcr.io/txn2/mcp-data-platform:latest
docker run -v /path/to/platform.yaml:/etc/mcp/platform.yaml ghcr.io/txn2/mcp-data-platform:latest --config /etc/mcp/platform.yaml
```

## Client Setup

### Claude Code

```bash
claude mcp add mcp-data-platform -- mcp-data-platform --config /path/to/platform.yaml
```

### Claude Desktop

Add to `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS):

```json
{
  "mcpServers": {
    "mcp-data-platform": {
      "command": "mcp-data-platform",
      "args": ["--config", "/path/to/platform.yaml"],
      "env": {
        "DATAHUB_TOKEN": "your-token"
      }
    }
  }
}
```

## Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--config` | Path to YAML configuration file | None |
| `--transport` | Transport protocol: `stdio` or `http` | `stdio` |
| `--address` | Listen address for HTTP transports | `:8080` |

---

# Configuration

Configuration uses YAML with environment variable expansion (`${VAR_NAME}`).

## Config Versioning

Every configuration file should include an `apiVersion` field as the first key. This enables safe schema evolution with deprecation warnings and migration tooling.

```yaml
apiVersion: v1

server:
  name: mcp-data-platform
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `apiVersion` | string | `v1` | Config schema version. Omitting defaults to `v1` for backward compatibility. |

**Supported versions**: `v1` (current)

### Migration Tool

Migrate config files to the latest version:

```bash
# From file to stdout
mcp-data-platform migrate-config --config platform.yaml

# From stdin to file
cat platform.yaml | mcp-data-platform migrate-config --output migrated.yaml

# Specify target version
mcp-data-platform migrate-config --config platform.yaml --target-version v1
```

The migration tool preserves comments and `${VAR}` environment variable references.

### Version Lifecycle

- **current**: Actively supported, no warnings
- **deprecated**: Still works, emits a warning at startup with migration guidance
- **removed**: Rejected at startup with an error pointing to the migration tool

## Minimal Configuration

```yaml
server:
  name: mcp-data-platform
  transport: stdio

toolkits:
  datahub:
    primary:
      url: https://datahub.example.com
      token: ${DATAHUB_TOKEN}

  trino:
    primary:
      host: trino.example.com
      port: 443
      user: ${TRINO_USER}
      password: ${TRINO_PASSWORD}
      ssl: true
      catalog: hive

injection:
  trino_semantic_enrichment: true
  datahub_query_enrichment: true
```

## Server Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `server.name` | string | `mcp-data-platform` | Platform identity (e.g., "ACME Corp Data Platform") - helps agents identify which business this MCP serves |
| `server.description` | string | - | Explains when to use this MCP - what business, products, or domains it covers. Agents use this to route questions to the right MCP server. |
| `server.tags` | array | `[]` | Keywords for discovery: company names, product names, business domains. Agents match these against user questions. |
| `server.agent_instructions` | string | - | Operational guidance: data conventions, required filters, unit conversions. Returned in `platform_info` response. |
| `server.prompts` | array | `[]` | Platform-level MCP prompts (name, description, content) registered via `prompts/list` |
| `server.transport` | string | `stdio` | Transport: `stdio` or `http` (`sse` accepted for backward compatibility) |
| `server.address` | string | `:8080` | Listen address for HTTP transports |
| `server.streamable.session_timeout` | duration | `30m` | How long an idle Streamable HTTP session persists before cleanup |
| `server.streamable.stateless` | bool | `false` | Disable session tracking (no `Mcp-Session-Id` validation) |

## Database Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `database.dsn` | string | - | PostgreSQL connection string |
| `database.max_open_conns` | int | 25 | Maximum open database connections |

Setting `dsn` enables audit logging, knowledge capture, session externalization, and OAuth persistence. Without it, these features degrade to in-memory or noop implementations.

## Config Store

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `config_store.mode` | string | `file` | Config storage mode: `file` or `database` |

**`file` mode**: Configuration loaded from YAML at startup, read-only. Admin API mutation endpoints return `409 Conflict`.

**`database` mode**: Configuration persisted to PostgreSQL. Requires `database.dsn`. Supports runtime mutations via the admin API. Bootstrap fields (`server`, `database`, `auth`, `admin`, `config_store`, `apiVersion`) always load from YAML.

## Tool Visibility Configuration

Reduce LLM token usage by hiding tools from `tools/list` responses. This is a visibility optimization, not a security boundary — persona-level tool filtering continues to gate `tools/call`.

```yaml
tools:
  allow:
    - "trino_*"
    - "datahub_*"
  deny:
    - "*_delete_*"
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `tools.allow` | array | `[]` | Tool name patterns to include in `tools/list` |
| `tools.deny` | array | `[]` | Tool name patterns to exclude from `tools/list` |

No patterns configured means all tools are visible. When both are set, allow is evaluated first, then deny removes from the result. Patterns use `filepath.Match` syntax (`*` matches any sequence of characters).

## Admin API Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `admin.enabled` | bool | `false` | Enable admin REST API |
| `admin.portal` | bool | `false` | Enable the admin web portal UI |
| `admin.persona` | string | `admin` | Persona required for admin access |
| `admin.path_prefix` | string | `/api/v1/admin` | URL prefix for admin endpoints |

When `admin.portal: true`, an interactive web dashboard is served at the admin path prefix. It provides audit log exploration, tool execution testing, and system monitoring.

## Audit Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `audit.enabled` | bool | `false` | Enable audit logging |
| `audit.log_tool_calls` | bool | `false` | Log MCP tool call events |
| `audit.retention_days` | int | 90 | Days to retain audit events |

Requires `database.dsn`. Both `enabled` and `log_tool_calls` must be `true` for tool call events to be recorded.

## Session Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `sessions.store` | string | `memory` | Backend: `memory` or `database` |
| `sessions.ttl` | duration | streamable session_timeout | Session lifetime |
| `sessions.idle_timeout` | duration | streamable session_timeout | Idle eviction threshold |
| `sessions.cleanup_interval` | duration | `1m` | Cleanup routine interval |

The `database` store requires `database.dsn`. Database-backed sessions survive restarts and support multi-replica deployments.

## Toolkit Configuration

### Trino

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `host` | string | required | Trino coordinator hostname |
| `port` | int | 8080/443 | Trino coordinator port |
| `user` | string | required | Trino username |
| `password` | string | - | Trino password |
| `catalog` | string | - | Default catalog |
| `schema` | string | - | Default schema |
| `ssl` | bool | false | Enable SSL/TLS |
| `timeout` | duration | 120s | Query timeout |
| `default_limit` | int | 1000 | Default row limit |
| `max_limit` | int | 10000 | Maximum row limit |
| `read_only` | bool | false | Restrict to read-only queries |

### DataHub

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `url` | string | required | DataHub GMS URL |
| `token` | string | - | DataHub access token |
| `timeout` | duration | 30s | API request timeout |
| `default_limit` | int | 10 | Default search limit |
| `max_limit` | int | 100 | Maximum search limit |

### S3

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `region` | string | us-east-1 | AWS region |
| `endpoint` | string | - | Custom S3 endpoint (for MinIO) |
| `access_key_id` | string | - | AWS access key ID |
| `secret_access_key` | string | - | AWS secret access key |
| `read_only` | bool | false | Restrict to read operations |

## Cross-Injection Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `trino_semantic_enrichment` | bool | false | Add DataHub context to Trino results |
| `datahub_query_enrichment` | bool | false | Add Trino availability to DataHub results |
| `s3_semantic_enrichment` | bool | false | Add DataHub context to S3 results |

## URN Mapping Configuration

When Trino catalog or platform names differ from DataHub metadata, configure bidirectional URN mapping:

```yaml
semantic:
  provider: datahub
  instance: primary
  urn_mapping:
    platform: postgres           # DataHub platform (e.g., postgres, mysql, trino)
    catalog_mapping:
      rdbms: warehouse           # Trino "rdbms" → DataHub "warehouse"
      iceberg: datalake          # Trino "iceberg" → DataHub "datalake"

query:
  provider: trino
  instance: primary
  urn_mapping:
    catalog_mapping:
      warehouse: rdbms           # DataHub "warehouse" → Trino "rdbms"
      datalake: iceberg          # DataHub "datalake" → Trino "iceberg"
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `semantic.urn_mapping.platform` | string | trino | Platform name in DataHub URNs |
| `semantic.urn_mapping.catalog_mapping` | map | {} | Map Trino catalogs to DataHub catalogs |
| `query.urn_mapping.catalog_mapping` | map | {} | Map DataHub catalogs to Trino catalogs (reverse) |

This translates URNs in both directions:
- **Trino → DataHub**: `rdbms.public.users` becomes `urn:li:dataset:(urn:li:dataPlatform:postgres,warehouse.public.users,PROD)`
- **DataHub → Trino**: URN with `warehouse.public.users` resolves to Trino table `rdbms.public.users`

---

# Authentication & Security

mcp-data-platform implements a **fail-closed** security model. Missing or invalid credentials deny access, never bypass. For the security architecture rationale, see: https://imti.co/mcp-defense/

## Transport Security

| Transport | Authentication | TLS | Why |
|-----------|---------------|-----|-----|
| stdio | Not needed | N/A | Local execution with your own credentials |
| HTTP | **Required** | Recommended | Shared server needs to identify users |

## Security Model

- **Required JWT Claims**: Tokens must include `sub` (subject) and `exp` (expiration)
- **Default-Deny Personas**: Users without explicit persona have no tool access
- **Prompt Injection Protection**: DataHub metadata is sanitized before exposure
- **Read-Only Enforcement**: Trino `read_only: true` blocks write queries at query level
- **Cryptographic Request IDs**: Secure random identifiers for audit trails

## stdio Transport (Local)

No MCP authentication required. The server runs on your machine using credentials you configured.

## HTTP Transport (Remote/Shared)

Authentication identifies who is making requests. Anonymous access is disabled by default.

- **OIDC**: Human users via Keycloak, Auth0, Okta
- **API Keys**: Service accounts, automation
- **OAuth 2.1**: Claude Desktop authentication via upstream IdP

### OAuth 2.1 for Claude Desktop

For Claude Desktop connecting to a remote MCP server, the built-in OAuth 2.1 server bridges authentication:

1. Claude Desktop calls `/oauth/authorize`
2. MCP server redirects to Keycloak
3. User logs in to Keycloak
4. Keycloak redirects back to MCP server's `/oauth/callback`
5. MCP server exchanges code with Keycloak, extracts user info
6. MCP server issues its own token to Claude Desktop

Configuration:
```yaml
oauth:
  enabled: true
  issuer: "https://mcp.example.com"
  signing_key: "${OAUTH_SIGNING_KEY}"  # Required: openssl rand -base64 32
  clients:
    - id: "claude-desktop"
      secret: "${CLAUDE_CLIENT_SECRET}"
      redirect_uris:
        - "http://localhost"
        - "http://127.0.0.1"
  upstream:
    issuer: "https://keycloak.example.com/realms/your-realm"
    client_id: "mcp-data-platform"
    client_secret: "${KEYCLOAK_CLIENT_SECRET}"
    redirect_uri: "https://mcp.example.com/oauth/callback"
```

---

# Audit Logging

Every tool call flows through the audit logging middleware, which records who called what, when, how long it took, and whether it succeeded. Audit logs are stored in PostgreSQL and automatically cleaned up based on a configurable retention period.

## Prerequisites

Audit logging requires:
1. A PostgreSQL database (version 13+)
2. The `database.dsn` configuration set
3. Both `audit.enabled` and `audit.log_tool_calls` set to `true`

## Configuration

```yaml
database:
  dsn: "${DATABASE_DSN}"

audit:
  enabled: true
  log_tool_calls: true
  retention_days: 90
```

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `database.dsn` | string | - | PostgreSQL connection string. Required for audit logging. |
| `audit.enabled` | bool | `false` | Master switch for audit logging. |
| `audit.log_tool_calls` | bool | `false` | Log every `tools/call` request. Both this and `enabled` must be `true`. |
| `audit.retention_days` | int | `90` | Days to keep audit logs before automatic cleanup. |

## What Gets Logged

Every tool call produces one row in `audit_logs` with these fields:

| Field | Type | Description |
|-------|------|-------------|
| `id` | VARCHAR(32) | Cryptographically random event ID (base64url, 16 bytes). |
| `timestamp` | TIMESTAMPTZ | When the tool call started. |
| `duration_ms` | INTEGER | Wall-clock time in milliseconds. |
| `request_id` | VARCHAR(255) | Request ID from auth middleware. |
| `user_id` | VARCHAR(255) | Authenticated user identity. |
| `user_email` | VARCHAR(255) | User email from OIDC claims. |
| `persona` | VARCHAR(100) | Resolved persona name. |
| `tool_name` | VARCHAR(255) | MCP tool name. |
| `toolkit_kind` | VARCHAR(100) | Toolkit type: `trino`, `datahub`, or `s3`. |
| `toolkit_name` | VARCHAR(100) | Toolkit instance name from config. |
| `connection` | VARCHAR(100) | Connection name from toolkit config. |
| `parameters` | JSONB | Tool call arguments with sensitive values redacted. |
| `success` | BOOLEAN | `true` if tool handler succeeded. |
| `error_message` | TEXT | Error description if `success` is `false`. |
| `created_date` | DATE | Partition key derived from `timestamp`. |

## Parameter Sanitization

Sensitive parameter keys are replaced with `[REDACTED]`: `password`, `secret`, `token`, `api_key`, `authorization`, `credentials`. Matching is case-sensitive and exact.

## Middleware Chain

The audit middleware sits inside the auth middleware in the execution chain:

```
Auth/Authz → Audit → Rules → Enrichment → Tool Handler
```

Auth creates the `PlatformContext` (user identity, persona, toolkit metadata). Audit reads it to build the audit event. Unauthorized requests are rejected before reaching audit, so only authenticated tool calls appear in audit logs.

Audit events are written **asynchronously** in a goroutine to avoid blocking tool responses. If the database write fails, the error is logged via `slog.Error` but the tool call still succeeds.

## Retention

A background goroutine runs every 24 hours and deletes rows older than `retention_days`. The table is partitioned by `created_date` for efficient cleanup.

---

# Cross-Injection

When you query a Trino table, you get DataHub context in the response. When you search DataHub, you see which datasets are queryable. No extra calls.

## The Problem It Solves

Without cross-injection:
1. Query a table
2. Search DataHub for that table
3. Get entity details for owners and tags
4. Check deprecation status
5. Look up quality score

Five calls to understand one table. With cross-injection, step 1 gives you everything.

## What Gets Injected

| When you use | You also get |
|--------------|--------------|
| Trino | DataHub metadata (owners, tags, quality, deprecation) |
| DataHub search | Which datasets are queryable in Trino |
| S3 | DataHub metadata for matching datasets |

## Semantic Context (added to Trino/S3 results)

```json
{
  "semantic_context": {
    "description": "Customer orders with line items and payment info",
    "owners": [{"name": "Data Team", "type": "group"}],
    "tags": ["pii", "financial"],
    "domain": {"name": "Sales"},
    "quality_score": 0.92,
    "deprecation": {
      "deprecated": true,
      "note": "Use orders_v2 instead"
    }
  }
}
```

## Query Context (added to DataHub results)

```json
{
  "query_context": {
    "urn:li:dataset:orders": {
      "queryable": true,
      "connection": "production",
      "table_identifier": {
        "catalog": "hive",
        "schema": "sales",
        "table": "orders"
      },
      "sample_query": "SELECT * FROM hive.sales.orders LIMIT 10"
    }
  }
}
```

## Lineage-Aware Column Inheritance

Downstream datasets (Elasticsearch indexes, Kafka topics) often lack documentation even when their upstream sources (Cassandra, PostgreSQL) are well-documented. The platform automatically inherits column metadata from upstream tables via DataHub lineage.

### How It Works

1. Query a table with undocumented columns
2. Platform checks DataHub lineage for upstream sources
3. Matches columns using column-level lineage, name matching, or configured aliases
4. Inherits descriptions, glossary terms, and tags from upstream
5. Returns enriched response with provenance tracking

### What Gets Inherited

| Metadata | Inherited When |
|----------|----------------|
| Descriptions | Target column has no description |
| Glossary Terms | Target column has no glossary terms |
| Tags | Target column has no tags |

### Match Methods

| Method | Description |
|--------|-------------|
| `column_lineage` | DataHub has explicit column-level lineage edges |
| `name_exact` | Column names match exactly |
| `name_transformed` | Names match after applying transforms (strip prefix/suffix) |
| `alias` | Explicit alias configuration bypasses lineage lookup |

### Configuration

```yaml
semantic:
  provider: datahub
  instance: primary

  lineage:
    enabled: true              # Enable lineage inheritance
    max_hops: 2                # Maximum upstream traversal depth (1-5)
    inherit:                   # Metadata types to inherit
      - glossary_terms
      - descriptions
      - tags
    conflict_resolution: nearest   # "nearest" (closest wins), "all" (merge), "skip"
    prefer_column_lineage: true    # Use column-level lineage when available

    # Column transforms for nested JSON paths
    column_transforms:
      - strip_prefix: "rxtxmsg.payload."
      - strip_prefix: "rxtxmsg.header."
      - strip_suffix: "_v2"

    # Explicit aliases when lineage isn't in DataHub
    aliases:
      - source: "cassandra.prod_fuse.system_sale"
        targets:
          - "elasticsearch.default.jakes-sale-*"
          - "elasticsearch.default.pos-sale-*"
        column_mapping:
          "rxtxmsg.payload.initial_net": "initial_net"

    cache_ttl: 10m             # Cache lineage graphs
    timeout: 5s                # Timeout for inheritance operation
```

### Response Format

When lineage inheritance is active, responses include column context with provenance:

```json
{
  "columns": [
    {"name": "rxtxmsg.payload.amount", "type": "DOUBLE"}
  ],
  "semantic_context": {
    "description": "Elasticsearch index for sales data",
    "urn": "urn:li:dataset:elasticsearch.default.jakes-sale-2025"
  },
  "column_context": {
    "rxtxmsg.payload.amount": {
      "description": "Net sale amount before adjustments",
      "glossary_terms": [
        {"urn": "urn:li:glossaryTerm:NetSaleAmount", "name": "Net Sale Amount"}
      ],
      "tags": ["financial"],
      "is_pii": false,
      "inherited_from": {
        "source_dataset": "urn:li:dataset:cassandra.prod_fuse.system_sale",
        "source_column": "initial_net",
        "hops": 1,
        "match_method": "name_transformed"
      }
    }
  },
  "inheritance_sources": [
    "urn:li:dataset:cassandra.prod_fuse.system_sale"
  ]
}
```

### Use Cases

**Elasticsearch indexes**: JSON documents from Cassandra have nested paths like `rxtxmsg.payload.amount`. Configure `strip_prefix: "rxtxmsg.payload."` to match upstream column `amount`.

**Kafka topics**: Event streams derived from source tables. Column-level lineage (if available) provides precise mapping.

**Data lakes**: Parquet files derived from operational databases. Aliases provide explicit mapping when lineage isn't tracked.

## Session Metadata Deduplication

When Trino enrichment is enabled, every tool call targeting a table receives ~2KB of semantic metadata. In a typical session querying the same table multiple times, this wastes LLM context tokens with repeated information.

Session dedup tracks which tables have been enriched per client session. First call gets full metadata; repeat calls within the TTL get reduced content based on the configured mode.

### Configuration

```yaml
injection:
  trino_semantic_enrichment: true
  session_dedup:
    enabled: true          # Default: true
    mode: reference        # reference (default), summary, none
    entry_ttl: 5m          # Defaults to semantic.cache.ttl
    session_timeout: 30m   # Defaults to server.streamable.session_timeout
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `enabled` | bool | `true` | Whether session dedup is active |
| `mode` | string | `reference` | What to send for repeat queries |
| `entry_ttl` | duration | semantic cache TTL | How long a table stays "already sent" |
| `session_timeout` | duration | streamable session timeout | Idle session cleanup |

### Modes

- **`reference`** (default): Repeat calls return `{"metadata_reference": {"tables": [...], "note": "Full semantic metadata was provided earlier..."}}`. Minimal tokens, LLM can refer back.
- **`summary`**: Repeat calls return table-level `semantic_context` without column details. LLM gets a reminder of ownership and tags.
- **`none`**: Repeat calls return raw tool results with no enrichment. Maximum token savings.

### Behavior

- Enabled by default when `trino_semantic_enrichment: true`
- Trino-only (DataHub/S3 enrichment is not deduplicated)
- In-memory state by default; persisted to database session store if `sessions.store: database`
- Each client session has independent dedup state
- SQL parsing tracks multiple tables independently (`JOIN` queries)

## Session Externalization

Externalize MCP session state to PostgreSQL for zero-downtime restarts and horizontal scaling.

### Configuration

```yaml
database:
  dsn: "${DATABASE_URL}"

sessions:
  store: database              # "memory" (default) or "database"
  ttl: 30m                     # session lifetime
  cleanup_interval: 1m         # cleanup routine frequency
```

When `store: database`:
- Platform forces `server.streamable.stateless: true` on the SDK
- Sessions are managed in PostgreSQL via `SessionAwareHandler`
- On shutdown, enrichment dedup state is flushed to the session store
- On startup, dedup state is restored from persisted sessions
- Multiple replicas share the same session store

Session hijack prevention: each session stores a SHA-256 hash of the creation token. Requests with a different token get HTTP 403.

---

# Tools API Reference

## Trino Tools

### trino_query

Execute a SQL query against Trino.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | SQL query to execute |
| `limit` | integer | No | 1000 | Maximum rows to return |
| `connection` | string | No | first configured | Trino connection name |

### trino_explain

Get the execution plan for a SQL query.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | SQL query to explain |
| `connection` | string | No | first configured | Trino connection name |

### trino_list_catalogs

List available catalogs. Parameters: `connection` (optional).

### trino_list_schemas

List schemas in a catalog. Parameters: `catalog`, `connection` (both optional).

### trino_list_tables

List tables in a schema. Parameters: `catalog`, `schema`, `connection` (all optional).

### trino_describe_table

Get table schema and metadata.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `table` | string | Yes | - | Table name (can be `catalog.schema.table`) |
| `connection` | string | No | first configured | Trino connection name |

### trino_list_connections

List configured Trino connections. No parameters.

## DataHub Tools

### datahub_search

Search for entities in the catalog.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | Search query |
| `type` | string | No | - | Entity type: dataset, dashboard, chart, dataflow |
| `platform` | string | No | - | Platform filter: trino, snowflake, s3 |
| `limit` | integer | No | 10 | Maximum results |
| `connection` | string | No | first configured | DataHub connection name |

### datahub_get_entity

Get detailed entity information.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `urn` | string | Yes | - | Entity URN |
| `connection` | string | No | first configured | DataHub connection name |

### datahub_get_schema

Get dataset schema. Parameters: `urn` (required), `connection` (optional).

### datahub_get_lineage

Get data lineage. Parameters: `urn` (required), `direction` (upstream/downstream), `depth`, `connection`.

### datahub_get_queries

Get popular queries for a dataset. Parameters: `urn` (required), `limit`, `connection`.

### datahub_get_glossary_term

Get glossary term details. Parameters: `urn` (required), `connection`.

### datahub_list_tags

List available tags. Parameters: `limit`, `connection`.

### datahub_list_domains

List data domains. Parameters: `limit`, `connection`.

### datahub_list_data_products

List data products. Parameters: `domain`, `limit`, `connection`.

### datahub_get_data_product

Get data product details. Parameters: `urn` (required), `connection`.

### datahub_list_connections

List configured DataHub connections. No parameters.

## S3 Tools

### s3_list_buckets

List available S3 buckets. Parameters: `connection` (optional).

### s3_list_objects

List objects in a bucket.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `bucket` | string | Yes | - | Bucket name |
| `prefix` | string | No | - | Key prefix filter |
| `delimiter` | string | No | - | Delimiter for hierarchy |
| `max_keys` | integer | No | 1000 | Maximum objects to return |
| `connection` | string | No | first configured | S3 connection name |

### s3_get_object

Get object contents. Parameters: `bucket`, `key` (required), `connection`.

### s3_get_object_metadata

Get object metadata without downloading content. Parameters: `bucket`, `key` (required), `connection`.

### s3_presign_url

Generate a pre-signed URL. Parameters: `bucket`, `key` (required), `expires`, `connection`.

### s3_list_connections

List configured S3 connections. No parameters.

### s3_put_object

Upload an object. Only available when `read_only: false`. Parameters: `bucket`, `key`, `content` (required), `content_type`, `connection`.

### s3_delete_object

Delete an object. Only available when `read_only: false`. Parameters: `bucket`, `key` (required), `connection`.

### s3_copy_object

Copy an object. Only available when `read_only: false`. Parameters: `source_bucket`, `source_key`, `dest_bucket`, `dest_key` (required), `connection`.

## Knowledge Tools

### capture_insight

Record domain knowledge shared during a session.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `category` | string | Yes | - | correction, business_context, data_quality, usage_guidance, relationship, enhancement |
| `insight_text` | string | Yes | - | Knowledge to record (10-4000 chars) |
| `confidence` | string | No | medium | high, medium, low |
| `entity_urns` | array | No | [] | Related DataHub entity URNs (max 10) |
| `related_columns` | array | No | [] | Related columns (max 20) |
| `suggested_actions` | array | No | [] | Proposed catalog changes (max 5) |

### apply_knowledge

Review, synthesize, and apply captured insights to the data catalog. Admin-only. Requires `knowledge.apply.enabled: true`.

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `action` | string | Yes | bulk_review, review, synthesize, apply, approve, reject |
| `entity_urn` | string | Conditional | Required for review, synthesize, apply |
| `insight_ids` | array | Conditional | Required for approve, reject; optional for synthesize, apply |
| `changes` | array | Conditional | Required for apply |
| `confirm` | bool | No | Required when `require_confirmation` is true |
| `review_notes` | string | No | Notes for approve/reject actions |

---

# Personas

Personas control which tools a user can access. Map OIDC roles to personas for role-based access control.

## Configuration

```yaml
personas:
  definitions:
    analyst:
      display_name: "Data Analyst"
      roles: ["analyst", "data_engineer"]
      tools:
        allow: ["trino_*", "datahub_*"]
        deny: ["*_delete_*"]
    admin:
      display_name: "Administrator"
      roles: ["admin"]
      tools:
        allow: ["*"]
  default_persona: analyst
```

## Tool Filtering

Patterns support wildcards:
- `*` matches any sequence of characters
- `trino_*` matches all Trino tools
- `*_delete_*` matches any delete tool

Evaluation order: deny patterns are checked first, then allow patterns.

---

# Go Library

Import the platform as a library for custom MCP servers.

## When to Use

Use the library when you need to:
- Add tools - Your domain-specific operations
- Swap providers - Different semantic layer, different query engine
- Write middleware - Custom auth, logging, rate limiting
- Embed it - MCP inside a larger application

## Packages

| Package | What it does |
|---------|--------------|
| `pkg/platform` | Main entry point, orchestration |
| `pkg/toolkits/*` | Trino, DataHub, S3, Knowledge adapters |
| `pkg/semantic` | Semantic provider interface |
| `pkg/query` | Query provider interface |
| `pkg/middleware` | Request/response processing |
| `pkg/persona` | Role-based tool filtering |
| `pkg/auth` | OIDC and API key validation |
| `pkg/admin` | Admin REST API for knowledge management |

## Minimal Example

```go
package main

import (
    "log"
    "os"
    "github.com/txn2/mcp-data-platform/pkg/platform"
    "github.com/txn2/mcp-data-platform/pkg/toolkits/datahub"
)

func main() {
    p, err := platform.New(
        platform.WithServerName("my-data-platform"),
        platform.WithDataHubToolkit("primary", datahub.Config{
            URL:   os.Getenv("DATAHUB_URL"),
            Token: os.Getenv("DATAHUB_TOKEN"),
        }),
    )
    if err != nil {
        log.Fatal(err)
    }
    defer p.Close()
    p.Run()
}
```

---

# Knowledge Capture

Domain knowledge shared during AI sessions -- column meanings, data quality issues, business rules -- is captured, reviewed through a governance workflow, and written back to DataHub with changeset tracking and rollback.

## The Problem

Every organization has tribal knowledge about its data that lives in the heads of experienced team members. AI-assisted data exploration surfaces this knowledge in conversations, but without capture, it's lost when the session ends. Knowledge capture persists these insights for admin review and catalog write-back.

## How It Works

The system has two MCP tools and an Admin REST API:

- `capture_insight`: Records domain knowledge during sessions. Available to all personas when enabled. Creates insights with status `pending`.
- `apply_knowledge`: Admin-only tool for reviewing, approving, synthesizing, and applying insights to DataHub. Actions: `bulk_review`, `review`, `synthesize`, `apply`, `approve`, `reject`.
- Admin REST API: HTTP endpoints for managing insights and changesets outside the MCP protocol.

## Configuration

```yaml
knowledge:
  enabled: true
  apply:
    enabled: true
    datahub_connection: primary
    require_confirmation: true
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `knowledge.enabled` | bool | `false` | Enable the knowledge capture toolkit and `capture_insight` tool |
| `knowledge.apply.enabled` | bool | `false` | Enable the `apply_knowledge` tool for admin review and catalog write-back |
| `knowledge.apply.datahub_connection` | string | - | DataHub instance name for write-back operations |
| `knowledge.apply.require_confirmation` | bool | `false` | When true, the `apply` action requires `confirm: true` in the request |

Prerequisites: `database.dsn` must be configured. The `apply_knowledge` tool requires the admin persona.

## Insight Categories

| Category | Description | Example |
|----------|-------------|---------|
| `correction` | Fixes wrong metadata | "The amount column is gross margin, not revenue" |
| `business_context` | Business meaning not in metadata | "MRR counts active subscriptions only, not trials" |
| `data_quality` | Quality issues or limitations | "Timestamps before March 2024 are UTC; after that, America/Chicago" |
| `usage_guidance` | Tips for querying correctly | "Always filter status='active' to avoid soft-delete duplicates" |
| `relationship` | Dataset connections not in lineage | "customer_id in orders joins to the legacy CRM export" |
| `enhancement` | Suggested metadata improvements | "Tag sales_daily with its 6 AM CT refresh schedule" |

## capture_insight Tool

Records domain knowledge shared during a session.

**Parameters:**

| Parameter | Type | Required | Description |
|-----------|------|----------|-------------|
| `category` | string | Yes | One of: `correction`, `business_context`, `data_quality`, `usage_guidance`, `relationship`, `enhancement` |
| `insight_text` | string | Yes | The knowledge to record (10-4000 characters) |
| `confidence` | string | No | `high`, `medium` (default), or `low` |
| `entity_urns` | array | No | DataHub URNs this insight relates to (max 10) |
| `related_columns` | array | No | Columns related to this insight (max 20) |
| `suggested_actions` | array | No | Proposed catalog changes (max 5) |

**Suggested action types:** `update_description`, `add_tag`, `add_glossary_term`, `flag_quality_issue`, `add_documentation`

**Response:**

```json
{
  "insight_id": "a1b2c3d4e5f67890a1b2c3d4e5f67890",
  "status": "pending",
  "message": "Insight captured. It will be reviewed by a data catalog administrator."
}
```

The tool also registers an MCP prompt `knowledge_capture_guidance` that provides AI agents with guidance on when to capture insights.

## apply_knowledge Tool

Admin-only tool for reviewing and applying captured insights. Requires the `admin` persona.

### Actions

**bulk_review**: Summary of all pending insights grouped by entity.

```json
{"action": "bulk_review"}
```

**review**: Insights for a specific entity with current DataHub metadata.

```json
{"action": "review", "entity_urn": "urn:li:dataset:(urn:li:dataPlatform:trino,hive.sales.orders,PROD)"}
```

**approve/reject**: Transition insight status with optional review notes.

```json
{
  "action": "approve",
  "insight_ids": ["a1b2c3d4e5f67890", "f6e5d4c3b2a10987"],
  "review_notes": "Verified with data engineering team"
}
```

**synthesize**: Structured change proposals from approved insights.

```json
{"action": "synthesize", "entity_urn": "urn:li:dataset:(urn:li:dataPlatform:trino,hive.sales.orders,PROD)"}
```

**apply**: Write changes to DataHub with changeset tracking.

```json
{
  "action": "apply",
  "entity_urn": "urn:li:dataset:(urn:li:dataPlatform:trino,hive.sales.orders,PROD)",
  "changes": [
    {"change_type": "update_description", "target": "entity", "detail": "Order records with gross margin amounts (before returns)"},
    {"change_type": "add_tag", "target": "entity", "detail": "gross-margin"}
  ],
  "insight_ids": ["a1b2c3d4e5f67890"],
  "confirm": true
}
```

## Insight Lifecycle

```
pending -> approved -> applied
pending -> rejected
pending -> superseded
applied -> rolled_back
```

| Status | Description |
|--------|-------------|
| `pending` | Newly captured, awaiting review |
| `approved` | Reviewed and approved for application |
| `rejected` | Reviewed and rejected |
| `applied` | Changes written to DataHub |
| `superseded` | Replaced by newer insights for the same entity |
| `rolled_back` | Applied changes were reverted |

## Governance Workflow

1. User shares domain knowledge during an AI session
2. AI calls `capture_insight` to record it (status: pending)
3. Admin uses `bulk_review` to see pending insights
4. Admin reviews and approves/rejects via `approve`/`reject`
5. Admin calls `synthesize` to generate change proposals from approved insights
6. Admin calls `apply` to write changes to DataHub with changeset tracking
7. Source insights are marked as applied with a changeset reference
8. Changeset stores previous values for rollback if needed

This is a human-in-the-loop metadata curation workflow. Every change to the catalog goes through admin review and is tracked for auditability.

## Changeset Tracking

Every `apply` action creates a changeset record:

| Field | Description |
|-------|-------------|
| `id` | Unique changeset identifier |
| `target_urn` | The DataHub entity that was modified |
| `change_type` | Summary of change types applied |
| `previous_value` | Entity metadata before changes (for rollback) |
| `new_value` | Changes that were applied |
| `source_insight_ids` | Insights that produced this changeset |
| `applied_by` | User who applied the changes |
| `rolled_back` | Whether this changeset has been reverted |

## Persona Integration

Control access through persona tool filtering:

```yaml
personas:
  definitions:
    analyst:
      tools:
        allow: ["trino_*", "datahub_*", "capture_insight"]
        deny: ["apply_knowledge"]
    admin:
      tools:
        allow: ["*"]
    etl_service:
      tools:
        allow: ["trino_*"]
        deny: ["capture_insight", "apply_knowledge"]
```

## Admin REST API

HTTP endpoints for managing insights and changesets. All endpoints require admin authentication via API key (X-API-Key or Authorization: Bearer header) with the admin role.

### Insight Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/api/v1/admin/knowledge/insights` | List insights with filtering and pagination |
| GET | `/api/v1/admin/knowledge/insights/{id}` | Get a single insight by ID |
| PUT | `/api/v1/admin/knowledge/insights/{id}` | Update insight text, category, or confidence |
| PUT | `/api/v1/admin/knowledge/insights/{id}/status` | Approve or reject an insight |
| GET | `/api/v1/admin/knowledge/insights/stats` | Get insight statistics |

**Query parameters for listing:**

| Parameter | Type | Description |
|-----------|------|-------------|
| `status` | string | Filter by status (pending, approved, rejected, applied, superseded, rolled_back) |
| `category` | string | Filter by category |
| `entity_urn` | string | Filter by entity URN |
| `captured_by` | string | Filter by user who captured |
| `confidence` | string | Filter by confidence level |
| `since` | RFC3339 | Filter by creation time (after) |
| `until` | RFC3339 | Filter by creation time (before) |
| `page` | integer | Page number (1-based) |
| `per_page` | integer | Results per page (default 20, max 100) |

### Changeset Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/api/v1/admin/knowledge/changesets` | List changesets with filtering and pagination |
| GET | `/api/v1/admin/knowledge/changesets/{id}` | Get a single changeset by ID |
| POST | `/api/v1/admin/knowledge/changesets/{id}/rollback` | Rollback a changeset (restores previous metadata) |

**Query parameters for listing changesets:**

| Parameter | Type | Description |
|-----------|------|-------------|
| `entity_urn` | string | Filter by target entity URN |
| `applied_by` | string | Filter by user who applied |
| `rolled_back` | boolean | Filter by rollback status |
| `since` | RFC3339 | Filter by creation time (after) |
| `until` | RFC3339 | Filter by creation time (before) |
| `page` | integer | Page number (1-based) |
| `per_page` | integer | Results per page (default 20, max 100) |

## Database Schema

Knowledge capture uses two PostgreSQL tables (migrations 000006, 000007, 000008).

**knowledge_insights:**

| Column | Type | Description |
|--------|------|-------------|
| `id` | TEXT | Primary key (cryptographic random hex) |
| `created_at` | TIMESTAMPTZ | When the insight was captured |
| `session_id` | TEXT | MCP session that produced it |
| `captured_by` | TEXT | User who shared the knowledge |
| `persona` | TEXT | Active persona at capture time |
| `category` | TEXT | Insight category |
| `insight_text` | TEXT | The domain knowledge |
| `confidence` | TEXT | Confidence level |
| `entity_urns` | JSONB | Related DataHub entities |
| `related_columns` | JSONB | Related columns |
| `suggested_actions` | JSONB | Proposed catalog changes |
| `status` | TEXT | Current lifecycle status |
| `reviewed_by` | TEXT | Who reviewed the insight |
| `reviewed_at` | TIMESTAMPTZ | When it was reviewed |
| `review_notes` | TEXT | Reviewer comments |
| `applied_by` | TEXT | Who applied the insight |
| `applied_at` | TIMESTAMPTZ | When it was applied |
| `changeset_ref` | TEXT | Link to the changeset |

**knowledge_changesets:**

| Column | Type | Description |
|--------|------|-------------|
| `id` | TEXT | Primary key (cryptographic random hex) |
| `created_at` | TIMESTAMPTZ | When changes were applied |
| `target_urn` | TEXT | DataHub entity that was modified |
| `change_type` | TEXT | Type of changes applied |
| `previous_value` | JSONB | Metadata before changes |
| `new_value` | JSONB | Changes applied |
| `source_insight_ids` | JSONB | Insights that produced this |
| `approved_by` | TEXT | Who approved the changes |
| `applied_by` | TEXT | Who applied the changes |
| `rolled_back` | BOOLEAN | Whether changes were reverted |
| `rolled_back_by` | TEXT | Who reverted the changes |
| `rolled_back_at` | TIMESTAMPTZ | When changes were reverted |

---

# Admin Portal

The Admin Portal is a built-in web dashboard for monitoring and managing the platform. Enable with `admin.portal: true` in configuration.

## Dashboard

Real-time platform health overview with configurable time ranges (1h, 6h, 24h, 7d):

- System info bar: platform name, version, transport, config mode, enabled features
- Summary cards: total calls, success rate, avg duration, unique users/tools, enrichment rate, errors
- Activity timeline: tool call volume over time with error overlay
- Top Tools / Top Users: horizontal bar charts of most active tools and users
- Performance: response time percentiles (P50, P95, P99) and response sizes
- Recent Errors: clickable error list with detail drawer
- Knowledge Insights: summary statistics and category breakdown
- Connections: all configured toolkit connections with tool counts

## Tools

### Overview Tab
Connection grid showing toolkit type, name, and tools. Tool Inventory table with tool name, description, visibility status, kind, connection, and toolkit assignment.

### Explore Tab
Interactive tool execution environment:
- Tool browser grouped by connection with search filtering
- Dynamic parameter form generated from JSON schema
- Result display with rendered markdown tables and Raw JSON toggle
- Semantic context: cross-injection enrichment shown below results (owners, tags, glossary terms, column metadata, lineage)
- Execution history with duration, status, and replay

## Audit Log

Searchable, filterable event log with detail drawer showing full request metadata:
- Identity: user email, persona, session ID
- Execution: tool name, toolkit, connection, duration
- Status: success/failure, enrichment status
- Transport: HTTP/stdio, request/response sizes
- Parameters: full request parameters as JSON

## Knowledge

### Overview Tab
Insight statistics, status/confidence distribution charts, category breakdown, top entities by insight count, recent pending insights, and recent changesets.

### Insights Tab
Insight list with filtering by status, category, and confidence. Detail drawer with full metadata, suggested actions, related columns, editable review notes, and approve/reject actions available regardless of insight status.

## Local Development

Run with mock data (no backend): `cd admin-ui && VITE_MSW=true npm run dev`
Full-stack: `make dev-up` + `go run ./cmd/mcp-data-platform --config dev/platform.yaml` + `psql -f dev/seed.sql` + `cd admin-ui && npm run dev`

---

# MCP Apps

MCP Apps provide interactive UI components that enhance tool results. Instead of raw JSON, users see sortable tables, charts, and filters. The platform provides the infrastructure; you provide the HTML/JS/CSS apps.

## Overview

1. User calls a tool (e.g., `trino_query`)
2. MCP server returns results with a UI resource reference
3. Host fetches the HTML app and renders it in an iframe
4. App receives tool results via `postMessage` and displays interactive UI

## Configuration

```yaml
mcpapps:
  enabled: true
  apps:
    query_results:
      enabled: true
      assets_path: "/etc/mcp-apps/query-results"
      tools:
        - trino_query
      csp:
        resource_domains:
          - "https://cdn.jsdelivr.net"
```

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `enabled` | bool | No | Enable MCP Apps infrastructure (default: false) |
| `apps.<name>.assets_path` | string | Yes | Absolute path to app directory |
| `apps.<name>.tools` | array | Yes | Tools this app enhances |
| `apps.<name>.csp.resource_domains` | array | No | Allowed CDN origins |

## Example App

The repository includes an example app at `apps/query-results/` demonstrating sortable tables, charts, dark mode, and search/filter. Use it as-is, customize it, or write your own.

## Development

Prerequisites: Docker (only)

```bash
docker compose -f docker-compose.dev.yml up
```

This starts: Test Harness (localhost:8000), MCP Server (localhost:3001), MCP Inspector (localhost:6274), Trino (localhost:8090).

Workflow:
1. Open http://localhost:8000/test-harness.html
2. Edit `./apps/query-results/index.html`
3. Click Reload App, then Send Test Data
4. Changes appear immediately

## MCP Apps Protocol

- App initiates with `ui/initialize` (not host)
- Communication via `window.parent.postMessage()`
- Tool results delivered via `ui/notifications/tool-result`
- Resources use MIME type `text/html;profile=mcp-app`

### Protocol Messages

1. **ui/initialize** (Request): App sends on load to establish connection
   ```javascript
   window.parent.postMessage({
       jsonrpc: '2.0',
       id: 1,
       method: 'ui/initialize',
       params: {
           protocolVersion: '2025-01-09',
           appInfo: { name: 'My App', version: '1.0.0' },
           appCapabilities: {
               availableDisplayModes: ['inline', 'fullscreen', 'pip']
           }
       }
   }, '*');
   ```

2. **ui/notifications/initialized**: App confirms ready after receiving init response

3. **ui/notifications/tool-result**: Host sends tool results to app
   ```javascript
   // Structure:
   {
       method: 'ui/notifications/tool-result',
       params: {
           toolName: 'platform_info',
           content: [{ type: 'text', text: '{"name":"mcp-data-platform",...}' }]
       }
   }
   // Note: Tool output is JSON-encoded inside params.content[0].text
   ```

4. **ui/notifications/size-changed**: App tells host preferred dimensions

### Display Modes

| Mode | Description |
|------|-------------|
| `inline` | Rendered within chat flow (default) |
| `fullscreen` | Takes over entire viewport |
| `pip` | Picture-in-picture floating window |

## Tutorial

For a complete step-by-step guide to building an MCP App, see the [MCP Apps Tutorial](mcpapps/tutorial.md). The tutorial walks through building a `platform-info` app and covers:

- MCP Apps protocol and message flow
- Understanding tool result structure
- Using the test harness for development
- Styling with mock data and dark mode support
- Connecting to real MCP tools

---

# Troubleshooting

## Quick Diagnosis

| Symptom | Likely Cause | Solution |
|---------|--------------|----------|
| Server exits immediately | Configuration error | Validate YAML syntax |
| 401 Unauthorized | Invalid credentials | Check token/key |
| 403 Forbidden | Persona/tool mismatch | Check persona tool rules |
| No enrichment data | Injection misconfigured | Enable injection settings |
| Slow responses | Performance bottleneck | Enable caching |
| Connection refused | Service unreachable | Check connectivity |

## Common Error Codes

| Code | Meaning | Solution |
|------|---------|----------|
| `AUTH_ERROR` | Authentication failed | Check credentials, token expiration |
| `AUTHZ_ERROR` | Authorization failed | Check persona tool rules |
| `TOOLKIT_ERROR` | Toolkit operation failed | Check service connectivity |
| `PROVIDER_ERROR` | Provider operation failed | Check DataHub/Trino config |
| `CONFIG_ERROR` | Configuration invalid | Validate YAML, check env vars |
| `TIMEOUT_ERROR` | Operation timed out | Increase timeout, check service |

## Enable Debug Logging

```bash
export LOG_LEVEL=debug
mcp-data-platform --config platform.yaml
```

---

# Operating Modes

mcp-data-platform supports three operating modes based on available infrastructure. The mode is determined by whether `database.dsn` is set and the `config_store.mode` setting.

## Mode Comparison

| Aspect | Standalone | File + Database | Bootstrap + DB Config |
|--------|-----------|-----------------|----------------------|
| `database.dsn` | empty | set | set |
| `config_store.mode` | `file` (default) | `file` (default) | `database` |
| Config source | YAML file only | YAML file only | Bootstrap YAML + DB |
| Config mutations | blocked | blocked | enabled |
| Knowledge tools | hidden (not registered) | registered | registered |
| Knowledge admin API | 409 Conflict | available | available |
| Audit logging | noop (silent) | PostgreSQL | PostgreSQL |
| Audit admin API | 409 Conflict | available | available |
| Sessions | memory | database | database |
| OAuth | available (memory store) | available (DB store) | available (DB store) |
| Persona/auth key CRUD | read-only | read-only | enabled |
| Config import | blocked | blocked | enabled |

**Standalone (No Database)**: Lightest deployment. No external database. Features requiring persistence (audit, knowledge) run in noop mode, tools are hidden. Admin API database-dependent endpoints return 409 Conflict.

**Full-Config File + Database**: Production default. Complete YAML config, database for persistence. Audit, knowledge, session externalization all available. Config immutable at runtime.

**Bootstrap + Database Config**: Minimal YAML for connections, full config in PostgreSQL with versioning. Admin API enables runtime mutations. Bootstrap fields always override DB on restart.

## Config Store

```yaml
config_store:
  mode: file      # "file" (default) or "database"
```

- `file`: Config loaded from YAML, read-only. Admin mutation endpoints return 409.
- `database`: Config persisted to PostgreSQL `config_versions` table. Requires `database.dsn`. Supports import, export, history, and runtime mutations. Bootstrap fields always override.

---

# Admin API

REST endpoints for managing the platform outside the MCP protocol. Mounted under a configurable path prefix (default: `/api/v1/admin`).

## Authentication

All endpoints require admin persona authentication via `X-API-Key` or `Authorization: Bearer` headers. Errors use RFC 9457 Problem Details format.

## Interactive API Documentation

Swagger UI is served at `GET /api/v1/admin/docs/index.html`. The OpenAPI spec is auto-generated from source code annotations and available at `/api/v1/admin/docs/doc.json`. Regenerate with `make swagger`.

## System Endpoints

- `GET /system/info` — Platform identity, version, runtime feature availability, config mode
- `GET /tools` — All registered tools across toolkits
- `GET /connections` — Toolkit connections with their tools

## Config Endpoints

- `GET /config` — Current config as JSON (sensitive values redacted)
- `GET /config/mode` — Config store mode and read-only status
- `GET /config/export` — Downloadable YAML config (`?secrets=true` to include sensitive values)
- `POST /config/import` — Import YAML config (database mode only, 409 in file mode)
- `GET /config/history` — Config revision history (database mode only)

## Persona Endpoints

- `GET /personas` — List personas with tool counts
- `GET /personas/{name}` — Single persona with resolved tool list
- `POST /personas` — Create persona (database config mode only)
- `PUT /personas/{name}` — Update persona (database config mode only)
- `DELETE /personas/{name}` — Delete persona (database config mode only, cannot delete admin persona)

## Auth Key Endpoints

- `GET /auth/keys` — List API keys (values never exposed)
- `POST /auth/keys` — Generate new API key (database config mode only, key shown once)
- `DELETE /auth/keys/{name}` — Delete API key (database config mode only)

## Audit Endpoints

Requires `audit.enabled: true` and database. Returns 409 without database.

- `GET /audit/events` — Paginated audit events with filtering (user_id, tool_name, session_id, success, time range)
- `GET /audit/events/{id}` — Single audit event
- `GET /audit/stats` — Aggregate counts (total, success, failures)

## Knowledge Endpoints

Requires `knowledge.enabled: true` and database. Returns 409 without database.

- `GET /knowledge/insights` — List insights with filtering
- `GET /knowledge/insights/stats` — Insight statistics
- `GET /knowledge/insights/{id}` — Get single insight
- `PUT /knowledge/insights/{id}` — Update insight text/category
- `PUT /knowledge/insights/{id}/status` — Approve or reject
- `GET /knowledge/changesets` — List changesets
- `GET /knowledge/changesets/{id}` — Get single changeset
- `POST /knowledge/changesets/{id}/rollback` — Rollback changes

---

# Links

- GitHub: https://github.com/txn2/mcp-data-platform
- Documentation: https://txn2.github.io/mcp-data-platform/
- DataHub: https://datahubproject.io/
- MCP Specification: https://modelcontextprotocol.io/
- Security Article: https://imti.co/mcp-defense/
