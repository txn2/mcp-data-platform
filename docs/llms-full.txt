# mcp-data-platform

> MCP server for AI-assisted data exploration. DataHub semantic layer with optional Trino and S3. Cross-injection automatically enriches query results with business context—owners, tags, quality scores, deprecation warnings. Implements fail-closed security with OIDC/API key authentication, TLS for SSE transport, and prompt injection protection.

For the security architecture rationale, see: https://imti.co/mcp-defense/

---

# Overview

Your AI assistant can run SQL. But it doesn't know that `cust_id` contains PII, that the table was deprecated last month, or who to ask when something breaks.

mcp-data-platform fixes that. It connects AI assistants to your data infrastructure and adds business context from your semantic layer. Query a table and get its meaning, owners, quality scores, and deprecation warnings in the same response.

The only requirement is DataHub (https://datahubproject.io/). Add Trino (https://trino.io/) for SQL queries and S3 for object storage when you're ready.

## Key Features

- **Semantic-First**: DataHub is the foundation. Query a table, get its business context automatically—owners, tags, quality scores, deprecation warnings. No separate lookups.

- **Cross-Injection**: Trino results include DataHub metadata. DataHub searches show which datasets are queryable. Context flows between services automatically.

- **Enterprise Security**: Fail-closed authentication model, TLS enforcement for SSE, prompt injection protection, and read-only mode enforcement.

- **Built for Customization**: Add custom toolkits, providers, and middleware. The Go library exposes everything. Build the data platform your organization needs.

- **Personas**: Define who can use which tools. Analysts get read access. Admins get everything. Map from your identity provider's roles.

---

# The Data Stack: DataHub + Trino + S3

Modern data platforms need three things: meaning (what does the data represent?), access (how do I query it?), and storage (where does it live?). mcp-data-platform uses DataHub for meaning, Trino for access, and S3 for storage.

## DataHub: The Semantic Layer

DataHub (https://datahubproject.io/) is an open source data catalog from LinkedIn. It stores business context: descriptions, owners, tags, glossary terms, lineage, and quality scores.

**Problem it solves**: Data exists everywhere, but understanding what it means requires tribal knowledge. Column `cid` in one system is `customer_id` in another. That deprecated table still gets queried because nobody knows it's deprecated.

**Why DataHub**: Active community (10k+ GitHub stars), rich GraphQL/REST APIs, 50+ integrations (Trino, Snowflake, dbt, Airflow), real-time ingestion, full-text search, lineage tracking.

**For AI**: Without DataHub, an AI sees columns and types. With DataHub, it sees what the data means, who owns it, and whether it's reliable.

## Trino: Universal SQL Access

Trino (https://trino.io/, formerly PrestoSQL from Facebook) is a distributed SQL query engine. It runs SQL against data where it lives, without moving it first.

**Trino connects to everything**: PostgreSQL, MySQL, Oracle, Snowflake, BigQuery, Elasticsearch, MongoDB, S3, HDFS, Delta Lake, Iceberg, Kafka, and more.

**One SQL dialect**: Query across PostgreSQL, Elasticsearch, and S3 in one statement. The AI doesn't need to know where data lives. It's all SQL.

**Why Trino**: Battle-tested at Meta, Netflix, Uber, LinkedIn. Cost-based optimizer, standard ANSI SQL, federated queries without data movement.

## S3: The Universal Data Lake

S3 (Amazon S3, MinIO, or any S3-compatible service) is object storage for any data: files, Parquet, JSON, logs, ML models.

**Problem it solves**: Not all data is structured or in databases. Data platforms must handle structured (tables), semi-structured (JSON, Parquet), and unstructured (PDFs, images) data.

**Why S3**: Infinite scale at low cost, any file format, direct querying via Trino, object versioning, S3-compatible (AWS, MinIO, Ceph).

## Cross-injection fills the gaps

| Component | Answers | Limitation Alone |
|-----------|---------|------------------|
| DataHub | "What does this mean?" | Can't query the data |
| Trino | "What's in this table?" | Doesn't know business context |
| S3 | "What files exist?" | Just storage, no meaning |

Cross-injection wires them together:
- Trino + DataHub: Query a table → Get schema + owners + tags + deprecation + quality
- DataHub + Trino: Search DataHub → See which datasets are queryable with sample SQL
- S3 + DataHub: List objects → Get matching metadata and ownership

This stack is built for OLAP: S3 stores petabytes at low cost, Trino runs analytical queries across that data, DataHub adds business context.

---

## What's Included

| Toolkit | Tools | Required |
|---------|-------|----------|
| DataHub | 11 tools | Yes |
| Trino | 7 tools | No |
| S3 | 6-9 tools | No |

---

# Installation

## Prerequisites

- Go 1.24+ (for building from source)
- An MCP-compatible client (Claude Desktop, Claude Code, or custom)
- Access to Trino, DataHub, and/or S3 services you want to connect

## Installation Methods

### Go Install

```bash
go install github.com/txn2/mcp-data-platform/cmd/mcp-data-platform@latest
```

### Homebrew (macOS)

```bash
brew install txn2/tap/mcp-data-platform
```

### Docker

```bash
docker pull ghcr.io/txn2/mcp-data-platform:latest
docker run -v /path/to/platform.yaml:/etc/mcp/platform.yaml ghcr.io/txn2/mcp-data-platform:latest --config /etc/mcp/platform.yaml
```

## Client Setup

### Claude Code

```bash
claude mcp add mcp-data-platform -- mcp-data-platform --config /path/to/platform.yaml
```

### Claude Desktop

Add to `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS):

```json
{
  "mcpServers": {
    "mcp-data-platform": {
      "command": "mcp-data-platform",
      "args": ["--config", "/path/to/platform.yaml"],
      "env": {
        "DATAHUB_TOKEN": "your-token"
      }
    }
  }
}
```

## Command Line Options

| Option | Description | Default |
|--------|-------------|---------|
| `--config` | Path to YAML configuration file | None |
| `--transport` | Transport protocol: `stdio` or `sse` | `stdio` |
| `--address` | Listen address for SSE transport | `:8080` |

---

# Configuration

Configuration uses YAML with environment variable expansion (`${VAR_NAME}`).

## Minimal Configuration

```yaml
server:
  name: mcp-data-platform
  transport: stdio

toolkits:
  datahub:
    primary:
      url: https://datahub.example.com
      token: ${DATAHUB_TOKEN}

  trino:
    primary:
      host: trino.example.com
      port: 443
      user: ${TRINO_USER}
      password: ${TRINO_PASSWORD}
      ssl: true
      catalog: hive

injection:
  trino_semantic_enrichment: true
  datahub_query_enrichment: true
```

## Server Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `server.name` | string | `mcp-data-platform` | Server name in MCP handshake |
| `server.transport` | string | `stdio` | Transport: `stdio` or `sse` |
| `server.address` | string | `:8080` | Listen address for SSE |

## Toolkit Configuration

### Trino

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `host` | string | required | Trino coordinator hostname |
| `port` | int | 8080/443 | Trino coordinator port |
| `user` | string | required | Trino username |
| `password` | string | - | Trino password |
| `catalog` | string | - | Default catalog |
| `schema` | string | - | Default schema |
| `ssl` | bool | false | Enable SSL/TLS |
| `timeout` | duration | 120s | Query timeout |
| `default_limit` | int | 1000 | Default row limit |
| `max_limit` | int | 10000 | Maximum row limit |
| `read_only` | bool | false | Restrict to read-only queries |

### DataHub

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `url` | string | required | DataHub GMS URL |
| `token` | string | - | DataHub access token |
| `timeout` | duration | 30s | API request timeout |
| `default_limit` | int | 10 | Default search limit |
| `max_limit` | int | 100 | Maximum search limit |

### S3

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `region` | string | us-east-1 | AWS region |
| `endpoint` | string | - | Custom S3 endpoint (for MinIO) |
| `access_key_id` | string | - | AWS access key ID |
| `secret_access_key` | string | - | AWS secret access key |
| `read_only` | bool | false | Restrict to read operations |

## Cross-Injection Configuration

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `trino_semantic_enrichment` | bool | false | Add DataHub context to Trino results |
| `datahub_query_enrichment` | bool | false | Add Trino availability to DataHub results |
| `s3_semantic_enrichment` | bool | false | Add DataHub context to S3 results |

## URN Mapping Configuration

When Trino catalog or platform names differ from DataHub metadata, configure bidirectional URN mapping:

```yaml
semantic:
  provider: datahub
  instance: primary
  urn_mapping:
    platform: postgres           # DataHub platform (e.g., postgres, mysql, trino)
    catalog_mapping:
      rdbms: warehouse           # Trino "rdbms" → DataHub "warehouse"
      iceberg: datalake          # Trino "iceberg" → DataHub "datalake"

query:
  provider: trino
  instance: primary
  urn_mapping:
    catalog_mapping:
      warehouse: rdbms           # DataHub "warehouse" → Trino "rdbms"
      datalake: iceberg          # DataHub "datalake" → Trino "iceberg"
```

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `semantic.urn_mapping.platform` | string | trino | Platform name in DataHub URNs |
| `semantic.urn_mapping.catalog_mapping` | map | {} | Map Trino catalogs to DataHub catalogs |
| `query.urn_mapping.catalog_mapping` | map | {} | Map DataHub catalogs to Trino catalogs (reverse) |

This translates URNs in both directions:
- **Trino → DataHub**: `rdbms.public.users` becomes `urn:li:dataset:(urn:li:dataPlatform:postgres,warehouse.public.users,PROD)`
- **DataHub → Trino**: URN with `warehouse.public.users` resolves to Trino table `rdbms.public.users`

---

# Authentication & Security

mcp-data-platform implements a **fail-closed** security model. Missing or invalid credentials deny access—never bypass. For the security architecture rationale, see: https://imti.co/mcp-defense/

## Transport Security

| Transport | Authentication | TLS | Why |
|-----------|---------------|-----|-----|
| stdio | Not needed | N/A | Local execution with your own credentials |
| SSE | **Required** | Recommended | Shared server needs to identify users |

## Security Model

- **Required JWT Claims**: Tokens must include `sub` (subject) and `exp` (expiration)
- **Default-Deny Personas**: Users without explicit persona have no tool access
- **Prompt Injection Protection**: DataHub metadata is sanitized before exposure
- **Read-Only Enforcement**: Trino `read_only: true` blocks write queries at query level
- **Cryptographic Request IDs**: Secure random identifiers for audit trails

## stdio Transport (Local)

No MCP authentication required. The server runs on your machine using credentials you configured.

## SSE Transport (Remote/Shared)

Authentication identifies who is making requests. Anonymous access is disabled by default.

- **OIDC**: Human users via Keycloak, Auth0, Okta
- **API Keys**: Service accounts, automation
- **OAuth 2.1**: Claude Desktop authentication via upstream IdP

### OAuth 2.1 for Claude Desktop

For Claude Desktop connecting to a remote MCP server, the built-in OAuth 2.1 server bridges authentication:

1. Claude Desktop calls `/oauth/authorize`
2. MCP server redirects to Keycloak
3. User logs in to Keycloak
4. Keycloak redirects back to MCP server's `/oauth/callback`
5. MCP server exchanges code with Keycloak, extracts user info
6. MCP server issues its own token to Claude Desktop

Configuration:
```yaml
oauth:
  enabled: true
  issuer: "https://mcp.example.com"
  signing_key: "${OAUTH_SIGNING_KEY}"  # Required: openssl rand -base64 32
  clients:
    - id: "claude-desktop"
      secret: "${CLAUDE_CLIENT_SECRET}"
      redirect_uris:
        - "http://localhost"
        - "http://127.0.0.1"
  upstream:
    issuer: "https://keycloak.example.com/realms/your-realm"
    client_id: "mcp-data-platform"
    client_secret: "${KEYCLOAK_CLIENT_SECRET}"
    redirect_uri: "https://mcp.example.com/oauth/callback"
```

---

# Cross-Injection

When you query a Trino table, you get DataHub context in the response. When you search DataHub, you see which datasets are queryable. No extra calls.

## The Problem It Solves

Without cross-injection:
1. Query a table
2. Search DataHub for that table
3. Get entity details for owners and tags
4. Check deprecation status
5. Look up quality score

Five calls to understand one table. With cross-injection, step 1 gives you everything.

## What Gets Injected

| When you use | You also get |
|--------------|--------------|
| Trino | DataHub metadata (owners, tags, quality, deprecation) |
| DataHub search | Which datasets are queryable in Trino |
| S3 | DataHub metadata for matching datasets |

## Semantic Context (added to Trino/S3 results)

```json
{
  "semantic_context": {
    "description": "Customer orders with line items and payment info",
    "owners": [{"name": "Data Team", "type": "group"}],
    "tags": ["pii", "financial"],
    "domain": {"name": "Sales"},
    "quality_score": 0.92,
    "deprecation": {
      "deprecated": true,
      "note": "Use orders_v2 instead"
    }
  }
}
```

## Query Context (added to DataHub results)

```json
{
  "query_context": {
    "urn:li:dataset:orders": {
      "queryable": true,
      "connection": "production",
      "table_identifier": {
        "catalog": "hive",
        "schema": "sales",
        "table": "orders"
      },
      "sample_query": "SELECT * FROM hive.sales.orders LIMIT 10"
    }
  }
}
```

## Lineage-Aware Column Inheritance

Downstream datasets (Elasticsearch indexes, Kafka topics) often lack documentation even when their upstream sources (Cassandra, PostgreSQL) are well-documented. The platform automatically inherits column metadata from upstream tables via DataHub lineage.

### How It Works

1. Query a table with undocumented columns
2. Platform checks DataHub lineage for upstream sources
3. Matches columns using column-level lineage, name matching, or configured aliases
4. Inherits descriptions, glossary terms, and tags from upstream
5. Returns enriched response with provenance tracking

### What Gets Inherited

| Metadata | Inherited When |
|----------|----------------|
| Descriptions | Target column has no description |
| Glossary Terms | Target column has no glossary terms |
| Tags | Target column has no tags |

### Match Methods

| Method | Description |
|--------|-------------|
| `column_lineage` | DataHub has explicit column-level lineage edges |
| `name_exact` | Column names match exactly |
| `name_transformed` | Names match after applying transforms (strip prefix/suffix) |
| `alias` | Explicit alias configuration bypasses lineage lookup |

### Configuration

```yaml
semantic:
  provider: datahub
  instance: primary

  lineage:
    enabled: true              # Enable lineage inheritance
    max_hops: 2                # Maximum upstream traversal depth (1-5)
    inherit:                   # Metadata types to inherit
      - glossary_terms
      - descriptions
      - tags
    conflict_resolution: nearest   # "nearest" (closest wins), "all" (merge), "skip"
    prefer_column_lineage: true    # Use column-level lineage when available

    # Column transforms for nested JSON paths
    column_transforms:
      - strip_prefix: "rxtxmsg.payload."
      - strip_prefix: "rxtxmsg.header."
      - strip_suffix: "_v2"

    # Explicit aliases when lineage isn't in DataHub
    aliases:
      - source: "cassandra.prod_fuse.system_sale"
        targets:
          - "elasticsearch.default.jakes-sale-*"
          - "elasticsearch.default.pos-sale-*"
        column_mapping:
          "rxtxmsg.payload.initial_net": "initial_net"

    cache_ttl: 10m             # Cache lineage graphs
    timeout: 5s                # Timeout for inheritance operation
```

### Response Format

When lineage inheritance is active, responses include column context with provenance:

```json
{
  "columns": [
    {"name": "rxtxmsg.payload.amount", "type": "DOUBLE"}
  ],
  "semantic_context": {
    "description": "Elasticsearch index for sales data",
    "urn": "urn:li:dataset:elasticsearch.default.jakes-sale-2025"
  },
  "column_context": {
    "rxtxmsg.payload.amount": {
      "description": "Net sale amount before adjustments",
      "glossary_terms": [
        {"urn": "urn:li:glossaryTerm:NetSaleAmount", "name": "Net Sale Amount"}
      ],
      "tags": ["financial"],
      "is_pii": false,
      "inherited_from": {
        "source_dataset": "urn:li:dataset:cassandra.prod_fuse.system_sale",
        "source_column": "initial_net",
        "hops": 1,
        "match_method": "name_transformed"
      }
    }
  },
  "inheritance_sources": [
    "urn:li:dataset:cassandra.prod_fuse.system_sale"
  ]
}
```

### Use Cases

**Elasticsearch indexes**: JSON documents from Cassandra have nested paths like `rxtxmsg.payload.amount`. Configure `strip_prefix: "rxtxmsg.payload."` to match upstream column `amount`.

**Kafka topics**: Event streams derived from source tables. Column-level lineage (if available) provides precise mapping.

**Data lakes**: Parquet files derived from operational databases. Aliases provide explicit mapping when lineage isn't tracked.

---

# Tools API Reference

## Trino Tools

### trino_query

Execute a SQL query against Trino.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | SQL query to execute |
| `limit` | integer | No | 1000 | Maximum rows to return |
| `connection` | string | No | first configured | Trino connection name |

### trino_explain

Get the execution plan for a SQL query.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | SQL query to explain |
| `connection` | string | No | first configured | Trino connection name |

### trino_list_catalogs

List available catalogs. Parameters: `connection` (optional).

### trino_list_schemas

List schemas in a catalog. Parameters: `catalog`, `connection` (both optional).

### trino_list_tables

List tables in a schema. Parameters: `catalog`, `schema`, `connection` (all optional).

### trino_describe_table

Get table schema and metadata.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `table` | string | Yes | - | Table name (can be `catalog.schema.table`) |
| `connection` | string | No | first configured | Trino connection name |

### trino_list_connections

List configured Trino connections. No parameters.

## DataHub Tools

### datahub_search

Search for entities in the catalog.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `query` | string | Yes | - | Search query |
| `type` | string | No | - | Entity type: dataset, dashboard, chart, dataflow |
| `platform` | string | No | - | Platform filter: trino, snowflake, s3 |
| `limit` | integer | No | 10 | Maximum results |
| `connection` | string | No | first configured | DataHub connection name |

### datahub_get_entity

Get detailed entity information.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `urn` | string | Yes | - | Entity URN |
| `connection` | string | No | first configured | DataHub connection name |

### datahub_get_schema

Get dataset schema. Parameters: `urn` (required), `connection` (optional).

### datahub_get_lineage

Get data lineage. Parameters: `urn` (required), `direction` (upstream/downstream), `depth`, `connection`.

### datahub_get_queries

Get popular queries for a dataset. Parameters: `urn` (required), `limit`, `connection`.

### datahub_get_glossary_term

Get glossary term details. Parameters: `urn` (required), `connection`.

### datahub_list_tags

List available tags. Parameters: `limit`, `connection`.

### datahub_list_domains

List data domains. Parameters: `limit`, `connection`.

### datahub_list_data_products

List data products. Parameters: `domain`, `limit`, `connection`.

### datahub_get_data_product

Get data product details. Parameters: `urn` (required), `connection`.

### datahub_list_connections

List configured DataHub connections. No parameters.

## S3 Tools

### s3_list_buckets

List available S3 buckets. Parameters: `connection` (optional).

### s3_list_objects

List objects in a bucket.

| Parameter | Type | Required | Default | Description |
|-----------|------|----------|---------|-------------|
| `bucket` | string | Yes | - | Bucket name |
| `prefix` | string | No | - | Key prefix filter |
| `delimiter` | string | No | - | Delimiter for hierarchy |
| `max_keys` | integer | No | 1000 | Maximum objects to return |
| `connection` | string | No | first configured | S3 connection name |

### s3_get_object

Get object contents. Parameters: `bucket`, `key` (required), `connection`.

### s3_get_object_metadata

Get object metadata without downloading content. Parameters: `bucket`, `key` (required), `connection`.

### s3_presign_url

Generate a pre-signed URL. Parameters: `bucket`, `key` (required), `expires`, `connection`.

### s3_list_connections

List configured S3 connections. No parameters.

### s3_put_object

Upload an object. Only available when `read_only: false`. Parameters: `bucket`, `key`, `content` (required), `content_type`, `connection`.

### s3_delete_object

Delete an object. Only available when `read_only: false`. Parameters: `bucket`, `key` (required), `connection`.

### s3_copy_object

Copy an object. Only available when `read_only: false`. Parameters: `source_bucket`, `source_key`, `dest_bucket`, `dest_key` (required), `connection`.

---

# Personas

Personas control which tools a user can access. Map OIDC roles to personas for role-based access control.

## Configuration

```yaml
personas:
  definitions:
    analyst:
      display_name: "Data Analyst"
      roles: ["analyst", "data_engineer"]
      tools:
        allow: ["trino_*", "datahub_*"]
        deny: ["*_delete_*"]
    admin:
      display_name: "Administrator"
      roles: ["admin"]
      tools:
        allow: ["*"]
  default_persona: analyst
```

## Tool Filtering

Patterns support wildcards:
- `*` matches any sequence of characters
- `trino_*` matches all Trino tools
- `*_delete_*` matches any delete tool

Evaluation order: deny patterns are checked first, then allow patterns.

---

# Go Library

Import the platform as a library for custom MCP servers.

## When to Use

Use the library when you need to:
- Add tools - Your domain-specific operations
- Swap providers - Different semantic layer, different query engine
- Write middleware - Custom auth, logging, rate limiting
- Embed it - MCP inside a larger application

## Packages

| Package | What it does |
|---------|--------------|
| `pkg/platform` | Main entry point, orchestration |
| `pkg/toolkits/*` | Trino, DataHub, S3 adapters |
| `pkg/semantic` | Semantic provider interface |
| `pkg/query` | Query provider interface |
| `pkg/middleware` | Request/response processing |
| `pkg/persona` | Role-based tool filtering |
| `pkg/auth` | OIDC and API key validation |

## Minimal Example

```go
package main

import (
    "log"
    "os"
    "github.com/txn2/mcp-data-platform/pkg/platform"
    "github.com/txn2/mcp-data-platform/pkg/toolkits/datahub"
)

func main() {
    p, err := platform.New(
        platform.WithServerName("my-data-platform"),
        platform.WithDataHubToolkit("primary", datahub.Config{
            URL:   os.Getenv("DATAHUB_URL"),
            Token: os.Getenv("DATAHUB_TOKEN"),
        }),
    )
    if err != nil {
        log.Fatal(err)
    }
    defer p.Close()
    p.Run()
}
```

---

# MCP Apps Development

MCP Apps provide interactive UI components that render alongside tool results in MCP-compatible hosts. This section covers local development setup.

## Prerequisites

- Go 1.21+
- Docker
- Node.js 18+

## Architecture

```
Browser (basic-host:8080) <--MCP--> mcp-data-platform:3001 <--SQL--> Trino:8090
```

## Quick Start (from project root)

1. Start Trino: `docker run -d --name trino-dev -p 8090:8080 trinodb/trino`
2. Start MCP: `go run ./cmd/mcp-data-platform --config configs/dev-mcpapps.yaml`
3. Clone basic-host (one-time): `git clone https://github.com/anthropics/ext-apps.git`
4. Start basic-host: `cd ext-apps/examples/basic-host && npm install && SERVERS='["http://localhost:3001"]' npm start`
5. Open http://localhost:8080 and call `trino_query` with the test JSON

## Test Query (call trino_query with this JSON)

```json
{"sql": "SELECT 1 as id, 'Product A' as name, 15000.50 as revenue UNION ALL SELECT 2, 'Product B', 23000.75 UNION ALL SELECT 3, 'Product C', 8500.25"}
```

## Key Files

- `pkg/mcpapps/queryresults/assets/index.html` - Interactive table UI
- `configs/dev-mcpapps.yaml` - Development configuration

## MCP Apps Protocol

- App initiates with `ui/initialize` (not host)
- Communication via `window.parent.postMessage()`
- Tool results delivered via `ui/context_update`
- Resources use MIME type `text/html;profile=mcp-app`

---

# Troubleshooting

## Quick Diagnosis

| Symptom | Likely Cause | Solution |
|---------|--------------|----------|
| Server exits immediately | Configuration error | Validate YAML syntax |
| 401 Unauthorized | Invalid credentials | Check token/key |
| 403 Forbidden | Persona/tool mismatch | Check persona tool rules |
| No enrichment data | Injection misconfigured | Enable injection settings |
| Slow responses | Performance bottleneck | Enable caching |
| Connection refused | Service unreachable | Check connectivity |

## Common Error Codes

| Code | Meaning | Solution |
|------|---------|----------|
| `AUTH_ERROR` | Authentication failed | Check credentials, token expiration |
| `AUTHZ_ERROR` | Authorization failed | Check persona tool rules |
| `TOOLKIT_ERROR` | Toolkit operation failed | Check service connectivity |
| `PROVIDER_ERROR` | Provider operation failed | Check DataHub/Trino config |
| `CONFIG_ERROR` | Configuration invalid | Validate YAML, check env vars |
| `TIMEOUT_ERROR` | Operation timed out | Increase timeout, check service |

## Enable Debug Logging

```bash
export LOG_LEVEL=debug
mcp-data-platform --config platform.yaml
```

---

# Links

- GitHub: https://github.com/txn2/mcp-data-platform
- Documentation: https://txn2.github.io/mcp-data-platform/
- DataHub: https://datahubproject.io/
- MCP Specification: https://modelcontextprotocol.io/
- Security Article: https://imti.co/mcp-defense/
